{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T01:23:19.646469Z",
     "iopub.status.busy": "2025-04-20T01:23:19.645910Z",
     "iopub.status.idle": "2025-04-20T01:23:24.610737Z",
     "shell.execute_reply": "2025-04-20T01:23:24.609596Z",
     "shell.execute_reply.started": "2025-04-20T01:23:19.646437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q langchain_community langchain langchain-google-genai faiss-cpu pypdf python-dotenv pandas scikit-learn sentence-transformers mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "“How do I build an AI assistant that can reliably answer questions over my own private documents (PDFs, manuals, reports, etc.)—without exposing that content publicly?”\n",
    "\n",
    "- Pain Point: Traditional keyword search or “copy‑paste into ChatGPT” is brittle, unindexed, often leaks context, and doesn’t trace back to your source.\n",
    "- GenAI Solution: Combine an embedding‑based vector store (for retrieval) with a generative LLM (for fluent answers). This let’s you:\n",
    "1. Index your docs into a FAISS vector database via embeddings\n",
    "2. Retrieve the top‑k most relevant passages at query time\n",
    "3. Generate a concise, grounded answer that cites its source chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Solves It (Step‑by‑Step)\n",
    "\n",
    "1. Environment & MLflow Setup\n",
    "- Installs/initializes all required libraries (LangChain, Gemini SDK, FAISS, MLflow).\n",
    "- Starts an MLflow experiment to track chunk‑size, overlap, query results, and future feedback.\n",
    "\n",
    "2. Document Ingestion & Chunking\n",
    "- Loads PDFs/TXT/DOCX/CSV with LangChain’s loaders.\n",
    "- Splits them into ~1 000‑token overlapping “chunks” for granular retrieval.\n",
    "\n",
    "3. Embedding & Vector Store\n",
    "- Uses GoogleGenerativeAIEmbeddings to turn each chunk into a vector.\n",
    "- Builds a FAISS index for sub‑second similarity search.\n",
    "\n",
    "4. Retrieval‑QA Chain Construction\n",
    "- Wires a RetrievalQA chain:\n",
    "    - Retriever: FAISS top‑k lookup\n",
    "    - LLM: Gemini chat model with a prompt that “Use ONLY this context”\n",
    "- Ensures the answer is always grounded in the retrieved snippets.\n",
    "\n",
    "5. Interactive Q&A Loop\n",
    "- Drops you into a REPL: type any question (or exit) and get back:\n",
    "    - A generative answer\n",
    "    - The exact source snippets that informed the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T01:23:32.378002Z",
     "iopub.status.busy": "2025-04-20T01:23:32.377672Z",
     "iopub.status.idle": "2025-04-20T01:23:32.386346Z",
     "shell.execute_reply": "2025-04-20T01:23:32.384681Z",
     "shell.execute_reply.started": "2025-04-20T01:23:32.377980Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Document processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import (\n",
    "    PyPDFLoader, \n",
    "    TextLoader, \n",
    "    CSVLoader,\n",
    "    UnstructuredWordDocumentLoader\n",
    ")\n",
    "\n",
    "# Gemini and embeddings\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Initialize MLflow & Define Core RAG Utilities\n",
    "\n",
    "In this section:\n",
    "\n",
    "- **Set up MLflow** for experiment and run tracking (`mlflow.set_experiment`).\n",
    "- **Load documents** of various formats (`.pdf`, `.txt`, `.docx`, `.csv`) into LangChain.\n",
    "- **Chunk** each document into overlapping text segments for better retrieval.\n",
    "- **Embed** these chunks with Gemini embeddings and build a FAISS vector store.\n",
    "- **Instantiate** a RetrievalQA chain using the Gemini LLM, wired to our vector store, so we can ask context‑grounded questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T01:24:14.611400Z",
     "iopub.status.busy": "2025-04-20T01:24:14.611037Z",
     "iopub.status.idle": "2025-04-20T01:24:14.631546Z",
     "shell.execute_reply": "2025-04-20T01:24:14.630426Z",
     "shell.execute_reply.started": "2025-04-20T01:24:14.611377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set up MLflow\n",
    "mlflow.set_experiment(\"document-qa-gemini\")\n",
    "\n",
    "# Document processing functions\n",
    "def load_document(file_path):\n",
    "    \"\"\"Load document based on file extension\"\"\"\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    \n",
    "    if file_extension.lower() == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_extension.lower() == '.txt':\n",
    "        loader = TextLoader(file_path)\n",
    "    elif file_extension.lower() == '.docx':\n",
    "        loader = UnstructuredWordDocumentLoader(file_path)\n",
    "    elif file_extension.lower() == '.csv':\n",
    "        loader = CSVLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "    \n",
    "    return loader.load()\n",
    "\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into chunks\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def create_vector_store(chunks, api_key):\n",
    "    \"\"\"Create a vector store from document chunks\"\"\"\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "def get_qa_chain(vector_store, api_key, model_name=\"gemini-1.5-pro\"):\n",
    "    \"\"\"Create a question-answering chain with Gemini model\"\"\"\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "    \n",
    "    llm = ChatGoogleGenerativeAI(model=model_name, temperature=0.2)\n",
    "    \n",
    "    template = \"\"\"\n",
    "    You are a helpful AI assistant trained to answer questions based on provided context.\n",
    "    Use only the following context to answer the question. If you don't know the answer based on \n",
    "    the context, say \"I don't have enough information to answer this question\" - don't make up information.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 4}),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define the DocumentQASystem Class\n",
    "\n",
    "Encapsulate the entire RAG workflow into a single, reusable class:\n",
    "\n",
    "- **`__init__`**  \n",
    "  - Configures the Gemini API key and MLflow experiment session  \n",
    "  - Initializes internal state (vector store, QA chain, session ID, chunk settings, history)\n",
    "\n",
    "- **`process_documents(file_paths, chunk_size, chunk_overlap)`**  \n",
    "  1. Loads each file (PDF, TXT, DOCX, CSV)  \n",
    "  2. Splits text into overlapping chunks for retrieval  \n",
    "  3. Creates embeddings & builds a FAISS vector store  \n",
    "  4. Instantiates a RetrievalQA chain with Gemini  \n",
    "  5. Logs processing parameters (chunk size, file count, etc.) to MLflow  \n",
    "  6. Returns the total number of chunks indexed  \n",
    "\n",
    "- **`ask_question(question, track_metrics=True)`**  \n",
    "  1. Records the user question in `conversation_history`  \n",
    "  2. Retrieves the top‐k chunks and generates an answer via the QA chain  \n",
    "  3. Formats and saves source snippets  \n",
    "  4. Appends the assistant’s response to history  \n",
    "  5. Optionally logs the question, answer, and source metadata to MLflow  \n",
    "  6. Returns a dict containing `answer` and `sources`\n",
    "\n",
    "Finally, create an instance of `DocumentQASystem`, point it at the law‑book PDF, and kick off document processing.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T01:24:21.390309Z",
     "iopub.status.busy": "2025-04-20T01:24:21.389935Z",
     "iopub.status.idle": "2025-04-20T01:25:33.055563Z",
     "shell.execute_reply": "2025-04-20T01:25:33.054367Z",
     "shell.execute_reply.started": "2025-04-20T01:24:21.390260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2833 chunks from 1 documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DocumentQASystem:\n",
    "    \"\"\"Main class to handle document Q&A with evaluation tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.vector_store = None\n",
    "        self.qa_chain = None\n",
    "        self.session_id = str(uuid.uuid4())\n",
    "        self.chunk_size = 1000\n",
    "        self.chunk_overlap = 200\n",
    "        self.conversation_history = []\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "        genai.configure(api_key=api_key)\n",
    "    \n",
    "    def process_documents(self, file_paths, chunk_size=1000, chunk_overlap=200):\n",
    "        \"\"\"Process multiple documents and create vector store\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        all_chunks = []\n",
    "        for file_path in file_paths:\n",
    "            documents = load_document(file_path)\n",
    "            chunks = chunk_documents(documents, chunk_size, chunk_overlap)\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        self.vector_store = create_vector_store(all_chunks, self.api_key)\n",
    "        self.qa_chain = get_qa_chain(self.vector_store, self.api_key)\n",
    "        \n",
    "        # Log document processing in MLflow\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_param(\"chunk_size\", chunk_size)\n",
    "            mlflow.log_param(\"chunk_overlap\", chunk_overlap)\n",
    "            mlflow.log_param(\"document_count\", len(file_paths))\n",
    "            mlflow.log_param(\"chunk_count\", len(all_chunks))\n",
    "            mlflow.log_param(\"session_id\", self.session_id)\n",
    "        \n",
    "        return len(all_chunks)\n",
    "    \n",
    "    def ask_question(self, question, track_metrics=True):\n",
    "        \"\"\"Ask a question about the loaded documents\"\"\"\n",
    "        if not self.vector_store or not self.qa_chain:\n",
    "            raise ValueError(\"Documents haven't been processed yet. Call process_documents first.\")\n",
    "        \n",
    "        # Add question to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Get answer from QA chain\n",
    "        response = self.qa_chain.invoke({\"query\": question})\n",
    "        answer = response[\"result\"]\n",
    "        source_docs = response[\"source_documents\"]\n",
    "        \n",
    "        # Format sources\n",
    "        sources = []\n",
    "        for i, doc in enumerate(source_docs):\n",
    "            source_text = doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "            sources.append(f\"Source {i+1}: {source_text}\")\n",
    "        \n",
    "        # Add response to conversation history\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": answer,\n",
    "            \"sources\": sources,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "        # Track metrics in MLflow\n",
    "        if track_metrics:\n",
    "            with mlflow.start_run():\n",
    "                mlflow.log_param(\"question\", question)\n",
    "                mlflow.log_param(\"session_id\", self.session_id)\n",
    "                mlflow.log_text(answer, \"answer.txt\")\n",
    "                mlflow.log_dict({\"sources\": [doc.page_content for doc in source_docs]}, \"sources.json\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "    \n",
    "qa_system = DocumentQASystem(api_key)\n",
    "\n",
    "law_book_path = \"/kaggle/input/international-law-handbook/book_1.pdf\"\n",
    "sample_files = [law_book_path]\n",
    "\n",
    "# Process documents\n",
    "chunk_count = qa_system.process_documents(sample_files)\n",
    "print(f\"Processed {chunk_count} chunks from {len(sample_files)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the Interactive Q&A Session\n",
    "\n",
    "Enter a simple REPL loop that:\n",
    "\n",
    "1. **Prompts** user to type a question about the indexed documents (or `exit` to quit)  \n",
    "2. **Invokes** `qa_system.ask_question(...)` to retrieve an answer and source snippets  \n",
    "3. **Prints** the model’s answer under “----- Answer -----”  \n",
    "4. **Lists** each retrieved source under “----- Sources -----” for full transparency  \n",
    "\n",
    "This lets user to explore the dataset law book in real time—just type a question and see instant, grounded responses!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T01:27:27.037633Z",
     "iopub.status.busy": "2025-04-20T01:27:27.037240Z",
     "iopub.status.idle": "2025-04-20T01:27:47.310686Z",
     "shell.execute_reply": "2025-04-20T01:27:47.309818Z",
     "shell.execute_reply.started": "2025-04-20T01:27:27.037607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question about law (or type 'exit' to quit):  Under what conditions can a treaty be terminated?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " your Question is:  Under what conditions can a treaty be terminated?\n",
      "\n",
      "--------------------------- Answer ---------------------------------\n",
      "A treaty can be terminated or a party can withdraw in the following ways:\n",
      "\n",
      "1. In conformity with the treaty's provisions.\n",
      "2. By consent of all parties after consultation with the contracting States and contracting organizations.\n",
      "3. If it conflicts with a new peremptory norm of general international law.\n",
      "4. Due to a fundamental change of circumstances.\n",
      "5. If the parties intended to admit the possibility of denunciation or withdrawal, or if such a right can be implied by the nature of the treaty, provided twelve months' notice is given.\n",
      "\n",
      "----- Sources -----\n",
      "Source 1: States and international organizations: treaties  87\n",
      "Article 56. Denunciation of or withdrawal from a treaty containing \n",
      "no provision regard ing termination, denunciation or withdrawal\n",
      "1. A treaty whi...\n",
      "---\n",
      "Source 2: States and international organizations: treaties  89\n",
      "4. If, under the foregoing paragraphs, a party may invoke a fundamental change of circum -\n",
      "stances as a ground for terminating or withdrawing from ...\n",
      "---\n",
      "Source 3: Convention on the law of treaties 47\n",
      "5. In cases falling under articles 51, 52 and 53, no separation of the provisions of the treaty is \n",
      "permitted.\n",
      "Article 45. Loss of a right to invoke a ground for i...\n",
      "---\n",
      "Source 4: A treaty is void if, at the time of its conclusion, it conflicts with a peremptory norm of general \n",
      "international law. For the purposes of the present Convention, a peremptory norm of general inter -\n",
      "...\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question about law (or type 'exit' to quit):  what is python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " your Question is:  what is python\n",
      "\n",
      "--------------------------- Answer ---------------------------------\n",
      "I don't have enough information to answer this question\n",
      "\n",
      "----- Sources -----\n",
      "Source 1: 1. For the purposes of this Convention, the term “torture” means any act by which severe pain \n",
      "or suffering, whether physical or mental, is intentionally inflicted on a person for such purposes as \n",
      "ob...\n",
      "---\n",
      "Source 2: tive and alternative modes, means and formats of communication, including accessible information \n",
      "and communication technology;\n",
      "“Language” includes spoken and signed languages and other forms of non-s...\n",
      "---\n",
      "Source 3: governed by international law, whether embodied in a single instrument or in two or more related \n",
      "instruments and whatever its particular designation;\n",
      "(b) “ratification,” “acceptance,” “approval” and ...\n",
      "---\n",
      "Source 4: offence was committed.\n",
      "article 12\n",
      "No one shall be subjected to arbitrary interference with his privacy, family, home or corre -\n",
      "spondence, nor to attacks upon his honour and reputation. Everyone has t...\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question about law (or type 'exit' to quit):  exit\n"
     ]
    }
   ],
   "source": [
    "# Interactive Q&A loop\n",
    "\n",
    "while True:\n",
    "    question = input(\"\\nAsk a question about law (or type 'exit' to quit): \")\n",
    "    if question.lower() == 'exit':\n",
    "        break\n",
    "        \n",
    "    result = qa_system.ask_question(question)\n",
    "    print(\"\\n your Question is: \",question)\n",
    "    print(\"\\n--------------------------- Answer ---------------------------------\")\n",
    "    print(result[\"answer\"])\n",
    "    print(\"\\n----- Sources -----\")\n",
    "    for source in result[\"sources\"]:\n",
    "        print(source)\n",
    "        print(\"---\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7192735,
     "sourceId": 11476474,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
